{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19715e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç OCR + Image Captioning Tool\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing your image... Please wait.\n",
      "\n",
      "======================================================================\n",
      "IMAGE CAPTION:\n",
      " a quote that says if you don ' t like you ' re, you ' re\n",
      "======================================================================\n",
      "OCR TEXT:\n",
      " Error reading image for OCR: [WinError 5] Access is denied\n",
      "======================================================================\n",
      "\n",
      "COMBINED DESCRIPTION:\n",
      " The image shows: a quote that says if you don ' t like you ' re, you ' re. The text in the image reads: Error reading image for OCR: [WinError 5] Access is denied\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Ensure transformers avoids TensorFlow\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:/Program Files/Tesseract-OCR\"\n",
    "\n",
    "\n",
    "def run_ocr(image_path):\n",
    "    \"\"\"\n",
    "    Perform OCR on the given image and return extracted text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error reading image for OCR: {str(e)}\"\n",
    "\n",
    "def generate_caption(image_path, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generate an image caption using the BLIP model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "        # Move model to device (CPU or GPU)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Open and process image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        out = model.generate(**inputs, max_new_tokens=50)\n",
    "        caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "        return caption\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating caption: {str(e)}\"\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"üîç OCR + Image Captioning Tool\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Get image path from user\n",
    "    image_path = input(\"Enter the path to your image file:\\n> \").strip()\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"ERROR: File not found ‚Üí {image_path}\")\n",
    "        return\n",
    "\n",
    "    # Ask for device choice\n",
    "    device = input(\"Run on CPU or CUDA? (type 'cpu' or 'cuda') [cpu]:\\n> \").strip().lower()\n",
    "    if device not in [\"cpu\", \"cuda\"]:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    print(\"\\nProcessing your image... Please wait.\\n\")\n",
    "\n",
    "    # Run OCR\n",
    "    ocr_text = run_ocr(image_path)\n",
    "\n",
    "    # Run captioning\n",
    "    caption = generate_caption(image_path, device=device)\n",
    "\n",
    "    # Display results\n",
    "    print(\"=\"*70)\n",
    "    print(\"IMAGE CAPTION:\\n\", caption)\n",
    "    print(\"=\"*70)\n",
    "    print(\"OCR TEXT:\\n\", ocr_text)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Combine into a single description\n",
    "    combined = f\"The image shows: {caption}. The text in the image reads: {ocr_text}\"\n",
    "    print(\"\\nCOMBINED DESCRIPTION:\\n\", combined)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c1ebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cpu\n",
      "\n",
      "--- üìù OCR Result ---\n",
      "If yOu\n",
      "\n",
      "youre walking,\n",
      "\n",
      "‚ÄîDolly Parton\n",
      "\n",
      "Prevention\n",
      "\n",
      "--- üñºÔ∏è Image Caption ---\n",
      "a quote that says if you don ' t like you ' re, you ' re\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Import Libraries\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ‚úÖ Load Image\n",
    "image_path = \"sample1.jpg\"\n",
    "assert os.path.exists(image_path), f\"‚ùå Image not found: {image_path}\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# ‚úÖ Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "\n",
    "# ‚úÖ Load processor and model ONCE globally\n",
    "processor = BlipProcessor.from_pretrained(\n",
    "    \"Salesforce/blip-image-captioning-base\", \n",
    "    use_fast=True\n",
    ")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model.to(device)\n",
    "\n",
    "# --- Part 1: OCR using Tesseract ---\n",
    "def perform_ocr(img: Image.Image) -> str:\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    return text.strip()\n",
    "\n",
    "# --- Part 2: Image Captioning using BLIP ---\n",
    "def generate_caption(img: Image.Image) -> str:\n",
    "    inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs)\n",
    "\n",
    "    caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# --- Run both tasks ---\n",
    "ocr_text = perform_ocr(image)\n",
    "caption_text = generate_caption(image)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- üìù OCR Result ---\")\n",
    "print(ocr_text if ocr_text else \"[No text detected]\")\n",
    "\n",
    "print(\"\\n--- üñºÔ∏è Image Caption ---\")\n",
    "print(caption_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
